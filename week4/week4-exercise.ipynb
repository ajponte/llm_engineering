{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Week 4 Exercise\n",
    "\n",
    "## Goal\n",
    "Create a script to translate Hirigana (ひりがな）to Katakana (カタカナ）to Kanji (漢字) using a specific LLM model (closed or open source).\n"
   ],
   "id": "68becebcfc07920b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T02:45:59.007289Z",
     "start_time": "2025-07-10T02:45:58.997557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import io\n",
    "import sys\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import google.generativeai\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display\n",
    "import gradio as gr\n",
    "import subprocess\n",
    "\n",
    "from sympy.strategies.core import switch"
   ],
   "id": "35a764d6b53810bd",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Domain-Level Exceptions**",
   "id": "cc054274dccea5a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T03:02:46.247294Z",
     "start_time": "2025-07-10T03:02:46.241646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EnvironmentException(Exception):\n",
    "    def __init__(self, message: str, cause: Exception | None = None):\n",
    "        self._message = message\n",
    "        self._cause = cause\n",
    "\n",
    "    @property\n",
    "    def message(self) -> str:\n",
    "        return self._message\n",
    "\n",
    "    @property\n",
    "    def cause(self) -> Exception | None:\n",
    "        return self._cause\n",
    "\n",
    "class ApiClientException(Exception):\n",
    "    \"\"\"Raise this exception upon errors with API client wrappers.\"\"\"\n",
    "    def __init__(self, message: str, cause: Exception | None = None):\n",
    "        self._message = message\n",
    "        self._cause = cause\n",
    "\n",
    "    @property\n",
    "    def message(self) -> str:\n",
    "        return self._message\n",
    "\n",
    "    @property\n",
    "    def cause(self) -> Exception | None:\n",
    "        return self._cause"
   ],
   "id": "90aa682c3d98a91a",
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Application-level configurations**",
   "id": "626d0d8381baec0c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T02:46:06.331636Z",
     "start_time": "2025-07-10T02:46:06.326775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DEFAULT_OPENAI_MODEL = \"gpt-4o\"\n",
    "DEFAULT_CLAUDE_MODEL = \"claude-3-5-sonnet-20240620\"\n",
    "# Toggle gradio auto-launching the UI.\n",
    "DEFAULT_GRADIO_UI_AUTO_LAUNCH = True\n",
    "\n",
    "_CONFIG = {\n",
    "    'OPENAI_MODEL': DEFAULT_OPENAI_MODEL,\n",
    "    'CLAUDE_MODEL': DEFAULT_CLAUDE_MODEL,\n",
    "    'GRADIO_UI_AUTO_LAUNCH': DEFAULT_GRADIO_UI_AUTO_LAUNCH\n",
    "}\n"
   ],
   "id": "21dba0501cb7cd6e",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Load API Keys**",
   "id": "975a0ff59ca1daaa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T02:46:07.858833Z",
     "start_time": "2025-07-10T02:46:07.854177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load API Keys.\n",
    "try:\n",
    "    load_dotenv(override=True)\n",
    "    os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
    "    os.environ['ANTHROPIC_API_KEY'] = os.getenv('ANTHROPIC_API_KEY', 'your-key-if-not-using-env')\n",
    "    os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN', 'your-key-if-not-using-env')\n",
    "except Exception as e:\n",
    "    error_message = \"Failure to setup environment variables, please check your configuration.\"\n",
    "    print(error_message)\n",
    "    raise EnvironmentException(message=error_message, cause=e)\n",
    "print(\"API Keys loaded!\")"
   ],
   "id": "9f5be36d66c9f2ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Keys loaded!\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Defining System Prompt**",
   "id": "891a96d3e5bf53ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T02:29:47.069314Z",
     "start_time": "2025-07-10T02:29:47.065321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "_SYSTEM_PROMPT = \"You are an assistant that translates Japanese hirigana (ひりがな) to its closest kanji (漢字). \"\n",
    "_SYSTEM_PROMPT = \"The input hirigana might be in romaji or actual hirigana characters. \"\n",
    "_SYSTEM_PROMPT += \"If you find multiple matches for the input hirigana, use your best reasoning skills to match the entire word or phrase. \"\n",
    "_SYSTEM_PROMPT += \"If you're unable to find any match for the input hirigana, let the user know.\""
   ],
   "id": "86303615394e7d15",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Setup System Model Interfaces**",
   "id": "904bb20a017e4d3a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T03:50:22.488501Z",
     "start_time": "2025-07-10T03:50:22.431005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from enum import StrEnum\n",
    "\n",
    "class LanguageModel(StrEnum):\n",
    "    GPT = \"gpt\"\n",
    "    CLAUDE = \"claude\"\n",
    "\n",
    "class RoleName(StrEnum):\n",
    "    SYSTEM = \"system\"\n",
    "    USER = \"user\"\n",
    "\n",
    "class OpenAiApiClient:\n",
    "    def __init__(\n",
    "        self,\n",
    "        system_prompt: str,\n",
    "        # todo: model version should derive from application-level config.\n",
    "        model_version: str = DEFAULT_OPENAI_MODEL,\n",
    "        user_prompt: str | None = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Create an OpenAI API client.\n",
    "\n",
    "        :param system_prompt: The prompt to use for prompting for the system prompt.\n",
    "                              This can be set with a later setter method.\n",
    "        :param user_prompt: Optional user prompt to initialize with.\n",
    "        \"\"\"\n",
    "        self._client = OpenAI()\n",
    "        # Setup a basic message hash for the model.\n",
    "        self._message_hashes = [\n",
    "            {\"role\": RoleName.SYSTEM, \"content\": system_prompt},\n",
    "            {\"role\": RoleName.USER, \"content\": user_prompt},\n",
    "        ]\n",
    "        self._model_version = model_version\n",
    "\n",
    "    def set_system_prompt(self, system_prompt: str) -> None:\n",
    "        \"\"\"Set the system prompt to a new value.\"\"\"\n",
    "        print(f\"Setting SystemPrompt to {system_prompt}\")\n",
    "        # Update internal message hash.\n",
    "        self._update_message_hash(\n",
    "            role=RoleName.SYSTEM, message=system_prompt\n",
    "        )\n",
    "\n",
    "    def set_user_prompt(self, user_prompt: str) -> None:\n",
    "        \"\"\"Set the user prompt to a new value.\"\"\"\n",
    "        print(f\"Setting UserPrompt to {user_prompt}\")\n",
    "        self._update_message_hash(\n",
    "            role=RoleName.USER, message=user_prompt\n",
    "        )\n",
    "\n",
    "\n",
    "    def chat_stream(self):\n",
    "        stream = self._execute_chat_stream()\n",
    "        try:\n",
    "            return self.__chunk_stream(stream)\n",
    "        except Exception as ex:\n",
    "            err_message = \"Failed to parse GPT chat stream.\"\n",
    "            print(err_message)\n",
    "            raise ApiClientException(message=err_message, cause=ex) from ex\n",
    "\n",
    "    def _execute_chat_stream(self):\n",
    "        \"\"\"Execute the chat stream via GPT client.\n",
    "\n",
    "        :return: The result of the chat stream.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\n",
    "                f\"Executing chat stream with \"\n",
    "                f\"model {self._model_version}, \"\n",
    "                f\" messages: {self._message_hashes}\"\n",
    "            )\n",
    "            stream = self._client.chat.completions.create(\n",
    "                model=self._model_version,\n",
    "                messages=self._message_hashes,\n",
    "                stream=True\n",
    "            )\n",
    "            return stream\n",
    "        except Exception as ex:\n",
    "            err_message = f\"Failed to execute GPT chat stream. Error: {ex}\"\n",
    "            print(err_message)\n",
    "            raise ApiClientException(message=err_message, cause=ex) from ex\n",
    "\n",
    "    def _prepare_reply(self, stream):\n",
    "        return self.__chunk_stream(stream)\n",
    "\n",
    "    def __chunk_stream(\n",
    "        self,\n",
    "        stream,\n",
    "        initial_response:str=\"\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Return the stream as chunks.\n",
    "\n",
    "        :param stream: The GPT stream to chunk.\n",
    "        :param initial_response: The initial string or reply to start the chunk response.\n",
    "        \"\"\"\n",
    "        for chunk in stream:\n",
    "            fragment = chunk.choices[0].delta.content or \"\"\n",
    "            initial_response += fragment\n",
    "            print(fragment, end='', flush=True)\n",
    "        return initial_response\n",
    "\n",
    "    def _update_message_hash(\n",
    "        self,\n",
    "        role: RoleName,\n",
    "        message: str\n",
    "    ) -> None:\n",
    "        \"\"\"Create a new message hash for the role or update the existing one.\"\"\"\n",
    "        # Remove the old hash and create the new one\n",
    "        self.__remove_message_hash(\n",
    "            role=RoleName.USER, message_hashes=self._message_hashes\n",
    "        )\n",
    "        self._message_hashes.append({'role': role, 'content': message})\n",
    "\n",
    "    @classmethod\n",
    "    def __remove_message_hash(\n",
    "        cls,\n",
    "        role: RoleName,\n",
    "        message_hashes: list[dict]\n",
    "    ) -> None:\n",
    "        \"\"\"Remove the existing message hash for a given role.\"\"\"\n",
    "        # Find the index of the first dictionary that contains the key.\n",
    "        # next() is used with a generator expression over enumerated items.\n",
    "        # If no such dictionary is found, 'None' is returned for 'index_to_remove'.\n",
    "        index_to_remove = next(\n",
    "            (i for i, d in enumerate(message_hashes) if role in d),\n",
    "            None\n",
    "        )\n",
    "\n",
    "        if index_to_remove is not None:\n",
    "            # If a dictionary is found, create a new list by concatenating\n",
    "            # the part before the found index and the part after it.\n",
    "            # This avoids modifying the original list and is a functional approach.\n",
    "            return message_hashes[:index_to_remove] + message_hashes[index_to_remove + 1:]\n",
    "        else:\n",
    "            # If no dictionary with the key is found, return a copy of the original list.\n",
    "            # Returning a copy is good practice in functional programming to ensure\n",
    "            # the original input is never mutated.\n",
    "            return list(message_hashes)\n",
    "\n",
    "\n"
   ],
   "id": "b64bb78af2400efe",
   "outputs": [],
   "execution_count": 137
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**User Prompt**",
   "id": "68a024ebb1c913fb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T03:50:23.524565Z",
     "start_time": "2025-07-10T03:50:23.519690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def user_prompt_for(hirigana: str) -> str:\n",
    "    \"\"\"\n",
    "    Return the user prompt for a given hirigana input.\n",
    "\n",
    "    param: hirigana: The input hirigana to translate to Kanji.\n",
    "    \"\"\"\n",
    "    if not hirigana:\n",
    "        raise ValueError(\"hirigana value is required!\")\n",
    "    _user_prompt = f\"Translate the following hirigana to kanji: {hirigana}\"\n",
    "    return _user_prompt\n"
   ],
   "id": "787cca37714afe92",
   "outputs": [],
   "execution_count": 138
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T03:50:24.832421Z",
     "start_time": "2025-07-10T03:50:23.997286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def execute_gpt(hirigana: str):\n",
    "    # Open GPT connection with system prompt.\n",
    "    gpt_client = OpenAiApiClient(system_prompt=_SYSTEM_PROMPT)\n",
    "    # Set user prompt for input hirigana\n",
    "    gpt_client.set_user_prompt(user_prompt_for(hirigana))\n",
    "    # Execute streaming\n",
    "    response = gpt_client.chat_stream()\n",
    "    print(f'response: {response}')\n",
    "    return response\n",
    "\n",
    "def test_execute_gpt():\n",
    "    execute_gpt(hirigana=\"かんじ\")\n",
    "\n",
    "test_execute_gpt()"
   ],
   "id": "bced9c8702fa410c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting UserPrompt to Translate the following hirigana to kanji: かんじ\n",
      "Executing chat stream with model gpt-4o,  messages: [{'role': <RoleName.SYSTEM: 'system'>, 'content': \"The input hirigana might be in romaji or actual hirigana characters. If you find multiple matches for the input hirigana, use your best reasoning skills to match the entire word or phrase. If you're unable to find any match for the input hirigana, let the user know.\"}, {'role': <RoleName.USER: 'user'>, 'content': None}, {'role': <RoleName.USER: 'user'>, 'content': 'Translate the following hirigana to kanji: かんじ'}]\n",
      "Failed to execute GPT chat stream. Error: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n"
     ]
    },
    {
     "ename": "ApiClientException",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mBadRequestError\u001B[39m                           Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[137]\u001B[39m\u001B[32m, line 69\u001B[39m, in \u001B[36mOpenAiApiClient._execute_chat_stream\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     64\u001B[39m \u001B[38;5;28mprint\u001B[39m(\n\u001B[32m     65\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mExecuting chat stream with \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     66\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mmodel \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m._model_version\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     67\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m messages: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m._message_hashes\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m     68\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m69\u001B[39m stream = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_client\u001B[49m\u001B[43m.\u001B[49m\u001B[43mchat\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcompletions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     70\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_model_version\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     71\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_message_hashes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     72\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[32m     73\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     74\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m stream\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/llms/lib/python3.11/site-packages/openai/_utils/_utils.py:287\u001B[39m, in \u001B[36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    286\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[32m--> \u001B[39m\u001B[32m287\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/llms/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:1087\u001B[39m, in \u001B[36mCompletions.create\u001B[39m\u001B[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001B[39m\n\u001B[32m   1086\u001B[39m validate_response_format(response_format)\n\u001B[32m-> \u001B[39m\u001B[32m1087\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1088\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m/chat/completions\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   1089\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1090\u001B[39m \u001B[43m        \u001B[49m\u001B[43m{\u001B[49m\n\u001B[32m   1091\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmessages\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1092\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmodel\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1093\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43maudio\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43maudio\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1094\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfrequency_penalty\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrequency_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1095\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfunction_call\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunction_call\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1096\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfunctions\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunctions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1097\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mlogit_bias\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogit_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1098\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mlogprobs\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1099\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmax_completion_tokens\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_completion_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1100\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmax_tokens\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1101\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmetadata\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1102\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmodalities\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodalities\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1103\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mn\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1104\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mparallel_tool_calls\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mparallel_tool_calls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1105\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mprediction\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprediction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1106\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mpresence_penalty\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpresence_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1107\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mreasoning_effort\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mreasoning_effort\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1108\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mresponse_format\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1109\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mseed\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1110\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mservice_tier\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mservice_tier\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1111\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstop\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1112\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstore\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstore\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1113\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstream\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1114\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstream_options\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1115\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtemperature\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1116\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtool_choice\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtool_choice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1117\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtools\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtools\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1118\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtop_logprobs\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_logprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1119\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtop_p\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1120\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43muser\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43muser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1121\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mweb_search_options\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mweb_search_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1122\u001B[39m \u001B[43m        \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1123\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[43m.\u001B[49m\u001B[43mCompletionCreateParamsStreaming\u001B[49m\n\u001B[32m   1124\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\n\u001B[32m   1125\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[43m.\u001B[49m\u001B[43mCompletionCreateParamsNonStreaming\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1126\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1127\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1128\u001B[39m \u001B[43m        \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\n\u001B[32m   1129\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1130\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m=\u001B[49m\u001B[43mChatCompletion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1131\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1132\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mStream\u001B[49m\u001B[43m[\u001B[49m\u001B[43mChatCompletionChunk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1133\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/llms/lib/python3.11/site-packages/openai/_base_client.py:1249\u001B[39m, in \u001B[36mSyncAPIClient.post\u001B[39m\u001B[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[39m\n\u001B[32m   1246\u001B[39m opts = FinalRequestOptions.construct(\n\u001B[32m   1247\u001B[39m     method=\u001B[33m\"\u001B[39m\u001B[33mpost\u001B[39m\u001B[33m\"\u001B[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001B[32m   1248\u001B[39m )\n\u001B[32m-> \u001B[39m\u001B[32m1249\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/llms/lib/python3.11/site-packages/openai/_base_client.py:1037\u001B[39m, in \u001B[36mSyncAPIClient.request\u001B[39m\u001B[34m(self, cast_to, options, stream, stream_cls)\u001B[39m\n\u001B[32m   1036\u001B[39m     log.debug(\u001B[33m\"\u001B[39m\u001B[33mRe-raising status error\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1037\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m._make_status_error_from_response(err.response) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1039\u001B[39m \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[31mBadRequestError\u001B[39m: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mApiClientException\u001B[39m                        Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[139]\u001B[39m\u001B[32m, line 14\u001B[39m\n\u001B[32m     11\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mtest_execute_gpt\u001B[39m():\n\u001B[32m     12\u001B[39m     execute_gpt(hirigana=\u001B[33m\"\u001B[39m\u001B[33mかんじ\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m14\u001B[39m \u001B[43mtest_execute_gpt\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[139]\u001B[39m\u001B[32m, line 12\u001B[39m, in \u001B[36mtest_execute_gpt\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m     11\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mtest_execute_gpt\u001B[39m():\n\u001B[32m---> \u001B[39m\u001B[32m12\u001B[39m     \u001B[43mexecute_gpt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhirigana\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mかんじ\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[139]\u001B[39m\u001B[32m, line 7\u001B[39m, in \u001B[36mexecute_gpt\u001B[39m\u001B[34m(hirigana)\u001B[39m\n\u001B[32m      5\u001B[39m gpt_client.set_user_prompt(user_prompt_for(hirigana))\n\u001B[32m      6\u001B[39m \u001B[38;5;66;03m# Execute streaming\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m response = \u001B[43mgpt_client\u001B[49m\u001B[43m.\u001B[49m\u001B[43mchat_stream\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mresponse: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m)\n\u001B[32m      9\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[137]\u001B[39m\u001B[32m, line 50\u001B[39m, in \u001B[36mOpenAiApiClient.chat_stream\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     49\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mchat_stream\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m---> \u001B[39m\u001B[32m50\u001B[39m     stream = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_execute_chat_stream\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     51\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m     52\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.__chunk_stream(stream)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[137]\u001B[39m\u001B[32m, line 78\u001B[39m, in \u001B[36mOpenAiApiClient._execute_chat_stream\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     76\u001B[39m err_message = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mFailed to execute GPT chat stream. Error: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mex\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m     77\u001B[39m \u001B[38;5;28mprint\u001B[39m(err_message)\n\u001B[32m---> \u001B[39m\u001B[32m78\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m ApiClientException(message=err_message, cause=ex) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mex\u001B[39;00m\n",
      "\u001B[31mApiClientException\u001B[39m: "
     ]
    }
   ],
   "execution_count": 139
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T03:17:49.421691Z",
     "start_time": "2025-07-10T03:17:49.419071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def translate(hirigana: str, model: str):\n",
    "    response = _select_model(hirigana, language_model=model)\n",
    "    for stream_so_far in response:\n",
    "        yield stream_so_far\n",
    "\n",
    "def _select_model(\n",
    "        hirigana: str,\n",
    "        language_model: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Choose the desired language model and execute the translation.\n",
    "    \"\"\"\n",
    "    if LanguageModel.GPT.casefold() == language_model.casefold():\n",
    "        return execute_gpt(hirigana=hirigana)\n",
    "    elif LanguageModel.CLAUDE.casefold() == language_model.casefold():\n",
    "        raise NotImplementedError(\"todo\")\n",
    "    raise ValueError(f\"Invalid model: {model}\")\n"
   ],
   "id": "663f23e9ccdc95e5",
   "outputs": [],
   "execution_count": 109
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### UI\n",
   "id": "2433543069856e33"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T03:17:54.479468Z",
     "start_time": "2025-07-10T03:17:54.046192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as ui:\n",
    "    # gr.Markdown('### Translate Hirigana (ひらがな) to Kanji（漢字）')\n",
    "    with gr.Row():\n",
    "        # Input text - Corrected variable name here\n",
    "        input_hirigana = gr.Textbox(label=\"hirigana word or phrase\", lines=10)\n",
    "        output_kanji = gr.Textbox(label=\"kanji word or phrase\", lines=10)\n",
    "    with gr.Row():\n",
    "        # Model selection\n",
    "        model = gr.Dropdown([\"GPT\", \"Claude\"], label=\"Select model\", value=\"GPT\")\n",
    "        # Translate Button\n",
    "        translate_btn = gr.Button(\"Translate\")\n",
    "    # Translation execute - Corrected input reference\n",
    "    translate_btn.click(translate, inputs=[input_hirigana, model], outputs=[output_kanji])\n",
    "\n",
    "# Launch UI.\n",
    "ui.launch(inbrowser=True)"
   ],
   "id": "62fc2ae75185361d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7873\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7873/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/2l/c7fg4dt56j1gxp8ls1jx0lnr0000gn/T/ipykernel_48352/3206488785.py\", line 81, in _execute_chat_stream\n",
      "    stream = self._client.chat.completions.create(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/llms/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/llms/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 1087, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/llms/lib/python3.11/site-packages/openai/_base_client.py\", line 1249, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/llms/lib/python3.11/site-packages/openai/_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null.\", 'type': 'invalid_request_error', 'param': 'messages.[1].content', 'code': None}}\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/queueing.py\", line 626, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/blocks.py\", line 2220, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/blocks.py\", line 1743, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/utils.py\", line 785, in async_iteration\n",
      "    return await anext(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/utils.py\", line 776, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/llms/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/llms/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/llms/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/utils.py\", line 759, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/utils.py\", line 923, in gen_wrapper\n",
      "    response = next(iterator)\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/2l/c7fg4dt56j1gxp8ls1jx0lnr0000gn/T/ipykernel_48352/434840167.py\", line 2, in translate\n",
      "    response = _select_model(hirigana, language_model=model)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/2l/c7fg4dt56j1gxp8ls1jx0lnr0000gn/T/ipykernel_48352/434840167.py\", line 14, in _select_model\n",
      "    return execute_gpt(hirigana)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/2l/c7fg4dt56j1gxp8ls1jx0lnr0000gn/T/ipykernel_48352/1657484140.py\", line 9, in execute_gpt\n",
      "    response = gpt_client.chat_stream()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/2l/c7fg4dt56j1gxp8ls1jx0lnr0000gn/T/ipykernel_48352/3206488785.py\", line 66, in chat_stream\n",
      "    stream = self._execute_chat_stream()\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/2l/c7fg4dt56j1gxp8ls1jx0lnr0000gn/T/ipykernel_48352/3206488785.py\", line 90, in _execute_chat_stream\n",
      "    raise ApiClientException(message=err_message, cause=e) from e\n",
      "ApiClientException\n"
     ]
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Deterministic Unit Tests\n",
    "Unit tests for deterministic logic.\n",
    "\n",
    "#### Prerequisites:\n",
    "- `pytest`"
   ],
   "id": "8e21e53969e701ab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T06:39:32.878690Z",
     "start_time": "2025-07-10T06:39:32.861974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import unittest\n",
    "from unittest.mock import MagicMock, patch\n",
    "import io\n",
    "import sys\n",
    "\n",
    "# Assume these are defined elsewhere or mock them for the test\n",
    "# For a bare-bones test, we'll define simple versions.\n",
    "class RoleName:\n",
    "    SYSTEM = \"system\"\n",
    "    USER = \"user\"\n",
    "    ASSISTANT = \"assistant\"\n",
    "\n",
    "DEFAULT_OPENAI_MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "# Placeholder for the OpenAI class.\n",
    "# In a real application, this would be 'from openai import OpenAI'.\n",
    "# We define it here so OpenAiApiClient can be defined without a NameError,\n",
    "# and then we can patch it correctly.\n",
    "class OpenAI:\n",
    "    \"\"\"A dummy OpenAI class to allow OpenAiApiClient definition.\"\"\"\n",
    "    def __init__(self):\n",
    "        pass # No actual initialization needed for the dummy\n",
    "\n",
    "# The class to be tested (provided by the user)\n",
    "class OpenAiApiClient:\n",
    "    def __init__(\n",
    "        self,\n",
    "        system_prompt: str,\n",
    "        # todo: model version should derive from application-level config.\n",
    "        model_version: str = DEFAULT_OPENAI_MODEL,\n",
    "        user_prompt: str | None = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Create an OpenAI API client.\n",
    "\n",
    "        :param system_prompt: The prompt to use for prompting for the system prompt.\n",
    "                              This can be set with a later setter method.\n",
    "        :param user_prompt: Optional user prompt to initialize with.\n",
    "        \"\"\"\n",
    "        # This is the line we want to mock.\n",
    "        # When the test runs, the 'OpenAI' class will be replaced by a MagicMock\n",
    "        # due to the @patch decorator in the test class.\n",
    "        self._client = OpenAI()\n",
    "        # Setup a basic message hash for the model.\n",
    "        self._message_hashes = [\n",
    "            {\"role\": RoleName.SYSTEM, \"content\": system_prompt},\n",
    "            {\"role\": RoleName.USER, \"content\": user_prompt},\n",
    "        ]\n",
    "        self._model_version = model_version\n",
    "\n",
    "# Bare-bones unit test class\n",
    "class TestOpenAiApiClient(unittest.TestCase):\n",
    "    \"\"\"\n",
    "    A bare-bones unit test class for the OpenAiApiClient.\n",
    "    This class uses unittest.mock.patch to mock the external OpenAI dependency,\n",
    "    allowing for isolated testing of the OpenAiApiClient's initialization logic.\n",
    "    \"\"\"\n",
    "\n",
    "    # The patch target is now 'OpenAI' in the current module (__main__ in Jupyter).\n",
    "    # This will replace the 'OpenAI' class defined above with a MagicMock.\n",
    "    @patch('__main__.OpenAI')\n",
    "    def setUp(self, MockOpenAI):\n",
    "        \"\"\"\n",
    "        Set up the test environment before each test method.\n",
    "        This method is called automatically by the unittest framework.\n",
    "        It initializes a mock for the OpenAI client and creates an instance\n",
    "        of OpenAiApiClient with predefined prompts.\n",
    "        \"\"\"\n",
    "        # MockOpenAI is the MagicMock replacing the actual OpenAI class.\n",
    "        # mock_openai_instance is the result of calling MockOpenAI(),\n",
    "        # which is what self._client will be assigned to in OpenAiApiClient's __init__.\n",
    "        self.mock_openai_instance = MockOpenAI.return_value\n",
    "\n",
    "        self.system_prompt = \"You are a helpful assistant.\"\n",
    "        self.user_prompt = \"Hello, world!\"\n",
    "        self.client = OpenAiApiClient(\n",
    "            system_prompt=self.system_prompt,\n",
    "            user_prompt=self.user_prompt\n",
    "        )\n",
    "\n",
    "    def test_initialization(self):\n",
    "        \"\"\"\n",
    "        Test that the OpenAiApiClient is initialized correctly.\n",
    "        This test verifies:\n",
    "        1. The internal _client attribute is an instance of the mocked OpenAI client.\n",
    "        2. The _message_hashes list is correctly populated with system and user prompts.\n",
    "        3. The _model_version is set to the default if not specified.\n",
    "        \"\"\"\n",
    "        # Assert that the _client attribute is the mocked OpenAI instance\n",
    "        # This confirms that self._client = OpenAI() inside the class\n",
    "        # indeed called the mocked OpenAI and got its return value.\n",
    "        self.assertEqual(self.client._client, self.mock_openai_instance)\n",
    "\n",
    "        # Assert that _message_hashes is correctly initialized\n",
    "        expected_message_hashes = [\n",
    "            {\"role\": RoleName.SYSTEM, \"content\": self.system_prompt},\n",
    "            {\"role\": RoleName.USER, \"content\": self.user_prompt},\n",
    "        ]\n",
    "        self.assertEqual(self.client._message_hashes, expected_message_hashes)\n",
    "\n",
    "        # Assert that _model_version is set to the default\n",
    "        self.assertEqual(self.client._model_version, DEFAULT_OPENAI_MODEL)\n",
    "\n",
    "    def test_initialization_no_user_prompt(self):\n",
    "        \"\"\"\n",
    "        Test initialization when no user prompt is provided.\n",
    "        Ensures that _message_hashes handles a None user_prompt correctly.\n",
    "        \"\"\"\n",
    "        # Create a new client instance without a user prompt\n",
    "        client_no_user = OpenAiApiClient(system_prompt=\"Another system prompt.\")\n",
    "\n",
    "        expected_message_hashes = [\n",
    "            {\"role\": RoleName.SYSTEM, \"content\": \"Another system prompt.\"},\n",
    "            {\"role\": RoleName.USER, \"content\": None}, # User prompt should be None\n",
    "        ]\n",
    "        self.assertEqual(client_no_user._message_hashes, expected_message_hashes)\n",
    "        self.assertEqual(client_no_user._model_version, DEFAULT_OPENAI_MODEL)\n",
    "\n",
    "    def test_initialization_custom_model_version(self):\n",
    "        \"\"\"\n",
    "        Test initialization with a custom model version.\n",
    "        Verifies that the provided model_version is correctly assigned.\n",
    "        \"\"\"\n",
    "        custom_model = \"gpt-4-turbo\"\n",
    "        client_custom_model = OpenAiApiClient(\n",
    "            system_prompt=\"System for custom model.\",\n",
    "            model_version=custom_model\n",
    "        )\n",
    "\n",
    "        self.assertEqual(client_custom_model._model_version, custom_model)\n",
    "        # Ensure other attributes are still correctly set\n",
    "        expected_message_hashes = [\n",
    "            {\"role\": RoleName.SYSTEM, \"content\": \"System for custom model.\"},\n",
    "            {\"role\": RoleName.USER, \"content\": None},\n",
    "        ]\n",
    "        self.assertEqual(client_custom_model._message_hashes, expected_message_hashes)\n",
    "\n",
    "\n",
    "# This block is modified to run tests in a Jupyter Notebook cell\n",
    "# It collects tests and runs them using TextTestRunner,\n",
    "# which prints results without trying to exit the interpreter.\n",
    "if __name__ == '__main__':\n",
    "    # Create a test suite from the TestOpenAiApiClient class\n",
    "    suite = unittest.TestSuite()"
   ],
   "id": "200602e956a937f6",
   "outputs": [],
   "execution_count": 144
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cde51cc59984288e"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
